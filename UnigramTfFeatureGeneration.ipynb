{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "import numpy as np\n",
    "from googletrans import Translator\n",
    "from nltk.tokenize import word_tokenize\n",
    "import codecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "hm_lines = 5000000\n",
    "translator = Translator()\n",
    "stopwords = codecs.open(\"hindi_stopwords.txt\", \"r\", encoding='utf-8', errors='ignore').read().split('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_lexicon(pos_hin, neg_eng, pos_eng, neg_hin):\n",
    "    lexicon = []\n",
    "    for file_name in [pos_hin, neg_eng, pos_eng, neg_hin]:\n",
    "        with codecs.open(file_name, 'r',encoding='utf-8',errors='ignore') as f:\n",
    "            contents = f.read()\n",
    "            for line in contents.split('$'):\n",
    "                data = line.strip('\\n')\n",
    "                if data:\n",
    "                    all_words = word_tokenize(data)\n",
    "                    lexicon += list(all_words)\n",
    "    lexicons = []\n",
    "    for word in lexicon:\n",
    "        if not word in stopwords:\n",
    "            lexicons.append(word)\n",
    "    word_counts = Counter(lexicons)  # it will return kind of dictionary\n",
    "    l2 = []\n",
    "    for word in word_counts:\n",
    "        if 60 > word_counts[word]:\n",
    "            l2.append(word)\n",
    "    return l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_handling(sample, lexicon, classification):\n",
    "    featureset = []\n",
    "    with codecs.open(sample, 'r', encoding=\"utf8\",errors='ignore') as f:\n",
    "        contents = f.read()\n",
    "        for line in contents.split('$'):\n",
    "            data = line.strip('\\n')\n",
    "            if data:\n",
    "                all_words = word_tokenize(data)\n",
    "                all_words_new = []\n",
    "                for word in all_words:\n",
    "                    if not word in stopwords:\n",
    "                        all_words_new.append(word)\n",
    "                features = np.zeros(len(lexicon))\n",
    "                for word in all_words_new:\n",
    "                    if word in lexicon:\n",
    "                        idx = lexicon.index(word)\n",
    "                        features[idx] = 1\n",
    "                features = list(features)\n",
    "                featureset.append([features, classification])\n",
    "    return featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_set_and_labels(pos_hin, neg_eng, pos_eng, neg_hin, test_size=0.2):\n",
    "    lexicon = create_lexicon(pos_hin, neg_eng, pos_eng, neg_neg_hin)\n",
    "    features = []\n",
    "    features += sample_handling(pos_hin, lexicon, 1)\n",
    "    features += sample_handling(neg_eng, lexicon, 0)\n",
    "    features += sample_handling(pos_eng, lexicon, 1)\n",
    "    features += sample_handling(neg_hin, lexicon, 0)\n",
    "    random.shuffle(features)\n",
    "    features = np.array(features)\n",
    "    #print(len(features))\n",
    "    testing_size = int((1 - test_size) * len(features))\n",
    "\n",
    "    x_train = list(features[:, 0][:testing_size])  # taking features array upto testing_size\n",
    "    y_train = list(features[:, 1][:testing_size])  # taking labels upto testing_size\n",
    "\n",
    "    x_test = list(features[:, 0][testing_size:])\n",
    "    y_test = list(features[:, 1][testing_size:])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_class(text, lexicon):\n",
    "    line = translator.translate(text, dest='hi').text\n",
    "    classifier = SupervisedDBNClassification.load('dbn.pkl')\n",
    "    predict_set = []\n",
    "    all_words = word_tokenize(line)\n",
    "    # all_words = [lemmatizer.lemmatize(i) for i in all_words]\n",
    "    features = np.zeros(len(lexicon))\n",
    "    for word in all_words:\n",
    "        if word in lexicon:\n",
    "            idx = lexicon.index(word)\n",
    "            features[idx] += 1\n",
    "    features = list(features)\n",
    "    predict_set.append(features)\n",
    "    predict_set = np.array(predict_set, dtype=np.float32)\n",
    "    predict_set = classifier.predict(predict_set)\n",
    "    #print(predict_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_feature_set_and_labels_simple(pos, neg, test_size=0.2):\n",
    "    lexicon = create_lexicon(pos, neg)\n",
    "    features = []\n",
    "    features += sample_handling(pos, lexicon, [1, 0])\n",
    "    features += sample_handling(neg, lexicon, [0, 1])\n",
    "    random.shuffle(features)\n",
    "    features = np.array(features)\n",
    "    #print(len(features))\n",
    "    testing_size = int((1 - test_size) * len(features))\n",
    "\n",
    "    x_train = list(features[:, 0][:testing_size])  \n",
    "    y_train = list(features[:, 1][:testing_size])  \n",
    "\n",
    "    x_test = list(features[:, 0][testing_size:])\n",
    "    y_test = list(features[:, 1][testing_size:])\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    create_lexicon('pos_hindi.txt', 'neg_hindi.txt', 'pos_eng.txt', 'neg_eng.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
